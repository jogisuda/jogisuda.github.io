---
title: 'Lie-Equivariant Quantum Graph Neural Networks'
date: 2024-07-15
permalink: /posts/2024/07/lie-eqgnn/
tags:
  - Geometric Quantum Machine Learning
---

![image](https://github.com/jogisuda/ML4SCI-2024/assets/22627105/e0430b68-4ff6-44d5-b388-79070fcb5172)

Discovering new phenomena in particle physics, like the Higgs boson, involves the identification of rare signals that can shed light into unanswered questions about our universe. At the Large Hadron Collider (LHC), particle collisions produce vast amounts of data. In this blog post, I explain a project I'm developing for the Google Summer of Code 2024, under the Machine Learning for Science (ML4SCI) organization. I am developing a Lie-Equivariant Quantum Graph Neural Network (Lie-EQGNN), a quantum model that not only is data efficient, but also has symmetry-preserving properties from any arbitrary Lie algebra that can be learned directly from the data. Given the limitations by noise of current quantum hardware, efficiently designing new parameterized circuit architectures is crucial. This blog post highlights an overview of the project. Aiming for the best intuitive explanation, the sections are split into three parts:

* Introduction
* Group theory review
* Jet data
* A primer on quantum/classical graph neural networks.
* Ways to achieve equivariance/invariance.

Introduction
======

The story of symmetries and machine learning goes a long way, back to Minsky and Papert. They showed that if a neural network is invariant to some group, then its output could be expressed as functions of the orbits of the group. In recent years, the field of <i>Geometric Deep Learning</i> was born, and it posits that in spite of the curse of dimensionality, the huge success of deep learning can be explained by two main inductive biases: symmetry and scale separation. By leveraging inherent symmetries in the data, the hypothesis space is effectively reduced, models can become more compact, and requiring less data for training - attributes that are particularly critical for quantum computing, given the current hardware limitations. 

In particle physics, symmetries underpin the fundamental laws governing the interactions and behaviors of subatomic particles: the Standard Model (SM), for example, is built upon symmetry principles such as gauge invariance, which dictate the interactions between particles and fields.

Recognizing the intertwined nature of symmetries and deep learning, a plethora of models have been developed that are invariant to different groups. Convolutional neural networks (CNNs), for example, which have revolutionized computer vision, are naturally invariant-equivariant to image translations. Transformer-based language models, on the other hand, exhibit permutation invariance. These architectures, when provided with appropriate data, are capable of learning stable latent representations under the action of their respective groups. An interesting question to be explored in quantum machine learning is the use of symmetries, especially in particle physics, which is replete with exotic symmetries. Quantum computers may have a unique advantage in this regard, as they can potentially capture quantum correlations more naturally and efficiently than classical computers. This can open the door to more accurate and insightful models that can better handle the complexities of particle interactions in huge datasets. The caveat is that symmetry is often corrupted by noisy observations, so imposing exact units of equivariance can be suboptimal. We propose to machine learn the Lie algebras behind particle observables, instead of imposing strict symmetry constraints. For this, we aim for three steps in this project:

* Symmetry discovery
* Invariant metric extraction
* Lie-Equivariant Quantum Graph Neural Network (Lie-EQGNN)

Group theory review
======
Symmetries are mathematically represented by algebraic structures called groups. Formally, a group $$\mathcal{G}$$ is a set endowed with a binary operation $\cdot$ $: \mathcal{G} \times \mathcal{G} \rightarrow{\mathcal{G}}$, such that:

* $a\cdot (b \cdot c) = (a\cdot b) \cdot c, \forall a, b, c \in \mathcal{G}$ (associativity)
* $\forall g \in \mathcal{G}, \exists$ $I$ such that $g\cdot I = I\cdot g$ (existence of the neutral element)
* $\forall g \in \mathcal{G}, \exists$ $g^{-1}$ such that $g\cdot g^{-1} = g^{-1}\cdot g = I$ (the inverse element always exists)

A Lie group is a continous group endowed with a topology. Specifically, it is a differentiable manifold $M$ that is also a group, and the operations of multiplication and inversion are also smooth maps. For example, all $2D$ rotations can be represented by a parameterized rotation matrix:

$$\begin{align*}R(\theta) = \begin{pmatrix}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{pmatrix}\end{align*}$$,

In high energy physics, particles moving in a laboratory at velocities neighboring the speed of light, like at the LHC, are described by special relativity. Its fundamental idea is that the laws of physics are the same for all observers in different inertial frames. Mathematically, the transformations between two inertial frames of reference are described by the so-called Lorentz group, mathematically represented as $O(1,3)$. Typically, one restricts the frames to be positively space-oriented and positively time-oriented, which gives rise to a subgroup called the proper orthochronous Lorentz group, or $SO^{+}(1,3)$ - but often just referred to as the Lorentz group, where its canonical generators representations are:

1. **Rotation Generators $L^{ij}$:**


$$\begin{align}
   (L^{12})^{\mu}_{\ \nu} = \begin{pmatrix}
   0 & 0 & 0 & 0 \\
   0 & 0 & 1 & 0 \\
   0 & -1 & 0 & 0 \\
   0 & 0 & 0 & 0
   \end{pmatrix},\;\;
    (L^{13})^{\mu}_{\ \nu} = \begin{pmatrix}
   0 & 0 & 0 & 0 \\
   0 & 0 & 0 & 1 \\
   0 & 0 & 0 & 0 \\
   0 & -1 & 0 & 0
   \end{pmatrix},\;\;
      (L^{23})^{\mu}_{\ \nu} = \begin{pmatrix}
   0 & 0 & 0 & 0 \\
   0 & 0 & 0 & 0 \\
   0 & 0 & 0 & 1 \\
   0 & 0 & -1 & 0
   \end{pmatrix}
\end{align}$$


2. **Boost Generators $L^{0i}$:**


$$\begin{align}
   (L^{01})^{\mu}_{\ \nu} = \begin{pmatrix}
   0 & 1 & 0 & 0 \\
   1 & 0 & 0 & 0 \\
   0 & 0 & 0 & 0 \\
   0 & 0 & 0 & 0
   \end{pmatrix},\;\;
   (L^{02})^{\mu}_{\ \nu} = \begin{pmatrix}
   0 & 0 & 1 & 0 \\
   0 & 0 & 0 & 0 \\
   1 & 0 & 0 & 0 \\
   0 & 0 & 0 & 0
   \end{pmatrix},\;\;
   (L^{03})^{\mu}_{\ \nu} = \begin{pmatrix}
   0 & 0 & 0 & 1 \\
   0 & 0 & 0 & 0 \\
   0 & 0 & 0 & 0 \\
   1 & 0 & 0 & 0
   \end{pmatrix}
\end{align}$$

This is one well-known symmetry group of special relativity, and our idea is to build some parameterized circuit equivariant to this group (or any we wish to discover from experimental data).

Equivariant Quantum Neural Networks
------
Given some group $\mathcal{G}$, the first way to achieve equivariance is to have a quantum neural network of the form $h_{\theta} = Tr[\rho \tilde{O}_{\theta}]$ such that:

$$\begin{align*}
\tilde{O}_{\theta} \in Comm(G) = \{A \in \mathbb{C}^{d\times d} / [A, R(g)] = 0 \text{ for all } g \in G\}
\end{align*}$$

To see why, we need to observe that the trace is cyclical, so:

$$\begin{align*}
    h_{\theta} (g\cdot \rho) = Tr[R(g)\rho R^{\dagger}(g)\tilde{O}_{\theta}] = Tr[\rho R^{\dagger}(g)\tilde{O}_{\theta}R(g)] &= Tr[\rho R^{\dagger}(g)R(g)\tilde{O}_{\theta}]\\ 
    &= Tr[\rho \tilde{O}_{\theta}]\\
    &= h_{\theta}(\rho).
\end{align*}$$

Essentially, we are using the Heisenberg picture, where we apply the time evolution to the measurement operator instead of the initial quantum state. When the observable is included in the commutant of $\mathcal{G}$, we can see how invariance is achieved.

The challenge with this approach is that it only works for finite-dimensional and compact groups, like $p4m$, $SO(3)$, etc. The Lorentz group is known to be continuous and non-compact, so it has no finite-dimensional unitary representation. Hence, the approach above is of no use for us. Hopefully, there is another way: instead of baking equivariance directly into the ansatze, we'll do it in the feature space.

Having a quick glance at the groups discussed above, we are getting a hint that the marriage between Physics and symmetries is deep. Indeed it is! To to quote Philip Anderson, who won the 1977 Nobel prize “for their fundamental theoretical investigations of the electronic structure of magnetic and disordered systems”:

> It is only slightly overstating the case to say that physics is the study of symmetry.

Indeed, in modern theories Beyond the Standard Model (BSM), physicists propose new Lagrangians with new invariants all the time. As we'll se in the next section, if we have a given invariant and want to discover its underlying group, some elegant approaches allow us to machine learn the corresponding symmetries.

Machine learned symmetries
======

Invariance
------
Any symmetry transformation is some action that leaves some property invariant, ie: rotations are length preserving. Formally, we say that we want to learn some transformation $f: \mathbb{R}^{N} \rightarrow \mathbb{R}^{N}$ that leaves some property given by $\mathbb{\phi}(x)$ invariant. From now on, we call $\mathbb{\phi}(x)$ an oracle. Such an oracle can be known numerically or analytically, the latter being the case of a particle theorist that wishes to discover the underlying group, for example, working on a new lagrangian beyond the standard model (BSM) that has a new invariant, and wishes to discover what is its underlying group. From a machine learning point of view, either numerical or analytical, the oracle is just a black box that ingests a vector and tells us the property we want to know - the Euclidean norm, the Minkowski norm, etc. Learning a symmetry, then, means that we aim to find some $f(x)$ such that:

$$\begin{equation*}
    \phi(f(x)) = \phi(x),
\end{equation*}$$

where $f$ can be modeled as a classical $1$-hidden layered neural network, or as a matrix.

Infinitesimality
------
Ultimately, $f$ is a machine-learned Lie algebra basis, meaning that it is a set of matrices that transform the data through the exponential map. To ensure that $f$ is an infinitesimal generator, we consider infinitesimal transformations $\delta \mathcal{F}$ in the neighborhood of the identity transformation $\mathbb{I}$:

$$\begin{equation*}
    \delta \mathcal{F} = \mathbb{I} + \epsilon \mathbb{G}_{\mathcal{W}},
\end{equation*}$$

where $\mathbb{G}_{\mathcal{W}}$ is our neural network parameterized $\mathcal{W}$, and $\epsilon$ is an infinitesimal parameter (in practice, we take it to be a small number of the order of $10^{-5}$).

Orthogonality
------
aaaa

Closure
------
aaaa


The invariant metric
======

Once the machine learned generators $L$ are found through the steps to be described here, we can extract the invariant metric tensor $J$ to further build invariant features, effectively baking the symmetries into our Quantum Graph Neural Network (QGNN). To do so, we can solve for:

$$\begin{equation}
    L_{i}\cdot J + J^{T}\cdot L_{i} = 0
\end{equation}$$

Numerically, a small push from zero is enough to avoid getting the trivial solution $J = 0$:

$$\begin{equation}
\underset{J}{\operatorname{argmin}} \sum_{i=1}^{c} ||L_{i}\cdot J + J^{T}\cdot L_{i}||^{2} -a||J||^{2}
\end{equation}$$

The proof for this equation can be found in [1]. The code below implements this procedure and was kindly given by the authors:

```python
# assuming you have the Lie generators stored under 'data'.
L = torch.load('data/top-tagging-L.pt')
a = 0.05
lr = 1.0
J = torch.randn(3, 3, requires_grad=True)
optimizer = torch.optim.LBFGS([J], lr=lr)

losses = []

def closure():
    optimizer.zero_grad()
    loss = 0.0
    for j in range(L.shape[0]):
        Lj = L[j]
        loss += torch.sum(torch.square(Lj.transpose(0, 1) @ J + J @ Lj))
    loss -= a * torch.linalg.norm(J)
    losses.append(loss.item())
    loss.backward()
    return loss

for i in range(1500):
    optimizer.step(closure)
    # if i <= 1000:
    #     scheduler.step()
print(J)
```

Jet data
======

In this work, we consider the task of determining whether a given jet was originated by a quark or a gluon, given its finnl state measurements. This is known as jet-tagging. For this, our dataset is constituted of point-clouds, which means that each jet is represented as a graph $\mathcal{G} = \{\mathcal{V,E}\}$, which is a set of nodes and edges, respectively. This is the natural data structure used by Graph Neural Networks.

![image](https://raw.githubusercontent.com/jogisuda/jogisuda.github.io/master/images/PFlow.png)

In a typical jet dataset, each node has the transverse momentum $p_T$, rapidity $\eta$, azimuthal angle $\phi$ and some other scalar like particle $ID$/mass - which identifies the particle measured in a multiplicity bin - as its features. Generally, the number of features is always constant, but the number of nodes in each jet may vary. 

Classical Graph Neural Networks
======

Graph Neural Networks (GNNs) are a class of neural networks designed to operate on graph-structured data. Unlike traditional neural networks that work on Euclidean data such as images or sequences, GNNs are capable of capturing the dependencies and relationships between nodes in a graph. The fundamental idea behind GNNs is to iteratively update the representation of each node by aggregating information from its neighbors, thereby enabling the network to learn both local and global graph structures.

Mathematically, a GNN can be described as follows: Let \( G = (V, E) \) be a graph where \( V \) is the set of nodes and \( E \) is the set of edges. Each node \( v \in V \) has an initial feature vector \( \mathbf{h}_v^{(0)} \). The node features are updated through multiple layers of the GNN. At each layer \( l \), the feature vector of node \( v \) is updated by aggregating the feature vectors of its neighbors \( \mathcal{N}(v) \) and combining them with its own feature vector. This can be expressed as:

\[
\mathbf{h}_v^{(l+1)} = \sigma \left( \mathbf{W}^{(l)} \mathbf{h}_v^{(l)} + \sum_{u \in \mathcal{N}(v)} \mathbf{W}_e^{(l)} \mathbf{h}_u^{(l)} \right)
\]

where \( \mathbf{W}^{(l)} \) and \( \mathbf{W}_e^{(l)} \) are learnable weight matrices, \( \sigma \) is a non-linear activation function, and \( \mathcal{N}(v) \) denotes the set of neighbors of node \( v \). This process is repeated for a fixed number of layers, allowing the network to capture higher-order neighborhood information.

The final node representations can be used for various downstream tasks such as node classification, link prediction, or graph classification.

Quantum Graph Neural Networks
======

Invariance/equivariance
======
