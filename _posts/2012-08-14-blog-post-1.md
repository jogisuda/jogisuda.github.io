---
title: 'Lie-Equivariant Quantum Graph Neural Networks'
date: 2024-07-15
permalink: /posts/2012/08/blog-post-1/
tags:
  - Geometric Quantum Machine Learning
---

![image](https://github.com/jogisuda/ML4SCI-2024/assets/22627105/e0430b68-4ff6-44d5-b388-79070fcb5172)

Discovering new phenomena in particle physics, like the Higgs boson, involves the identification of rare signals that can shed light into unanswered questions about our universe. At the Large Hadron Collider (LHC), particle collisions produce vast amounts of data. In this blog post, I explain a project I'm developing for the Google Summer of Code 2024, under the Machine Learning for Science (ML4SCI) organization. I am developing a Lie-Equivariant Quantum Graph Neural Network (Lie-EQGNN), a quantum model that not only is data efficient, but also has symmetry-preserving properties from any arbitrary Lie algebra that can be learned directly from the data. Given the limitations by noise of current quantum hardware, efficiently designing new parameterized circuit architectures is crucial. This blog post highlights an overview of the project. Aiming for the best intuitive explanation, the sections are split into three parts:

* Introduction
* Group theory review
* Jet data
* A primer on quantum/classical graph neural networks.
* Ways to achieve equivariance/invariance.

Introduction
======

The story of symmetries and machine learning goes a long way, back to Minsky and Papert. They showed that if a neural network is invariant to some group, then its output could be expressed as functions of the orbits of the group. In recent years, the field of <i>Geometric Deep Learning</i> was born, and it posits that in spite of the curse of dimensionality, the huge success of deep learning can be explained by two main inductive biases: symmetry and scale separation. By leveraging inherent symmetries in the data, the hypothesis space is effectively reduced, models can become more compact, and requiring less data for training - attributes that are particularly critical for quantum computing, given the current hardware limitations. 

In particle physics, symmetries underpin the fundamental laws governing the interactions and behaviors of subatomic particles: the Standard Model (SM), for example, is built upon symmetry principles such as gauge invariance, which dictate the interactions between particles and fields.

Recognizing the intertwined nature of symmetries and deep learning, a plethora of models have been developed that are invariant to different groups. Convolutional neural networks (CNNs), for example, which have revolutionized computer vision, are naturally invariant-equivariant to image translations. Transformer-based language models, on the other hand, exhibit permutation invariance. These architectures, when provided with appropriate data, are capable of learning stable latent representations under the action of their respective groups. An interesting question to be explored in quantum machine learning is the use of symmetries, especially in particle physics, which is replete with exotic symmetries. Quantum computers may have a unique advantage in this regard, as they can potentially capture quantum correlations more naturally and efficiently than classical computers. This can open the door to more accurate and insightful models that can better handle the complexities of particle interactions in huge datasets.

Group theory review
======
Symmetries are mathematically represented by algebraic structures called groups. Formally, a group $$\mathcal{G}$$ is a set endowed with a binary operation $\cdot$ $: \mathcal{G} \times \mathcal{G} \rightarrow{\mathcal{G}}$, such that:

* $a\cdot (b \cdot c) = (a\cdot b) \cdot c, \forall a, b, c \in \mathcal{G}$ (associativity)
* $\forall g \in \mathcal{G}, \exists$ $I$ such that $g\cdot I = I\cdot g$ (existence of the neutral element)
* $\forall g \in \mathcal{G}, \exists$ $g^{-1}$ such that $g\cdot g^{-1} = g^{-1}\cdot g = I$ (the inverse element always exists)

A Lie group is a continous group endowed with a topology. Specifically, it is a differentiable manifold $M$ that is also a group, and the operations of multiplication and inversion are also smooth maps. For example, all $2D$ rotations can be represented by a parameterized rotation matrix:

$$R(\theta) = \begin{pmatrix}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{pmatrix}$$

In high energy physics, particles moving in a laboratory at velocities neighboring the speed of light, like at the LHC, are described by special relativity. Its fundamental idea is that the laws of physics are the same for all observers in different inertial frames. Mathematically, the transformations between two inertial frames of reference are described by the so-called Lorentz group, mathematically represented as $O(1,3)$. Typically, one restricts the frames to be positively space-oriented and positively time-oriented, which gives rise to a subgroup called the proper orthochronous Lorentz group, or $SO^{+}(1,3)$ - but often just referred to as the Lorentz group. In notebook $1$, we used a Lie Generative Adversarial Network (LieGAN) to machine learn the underlying symmetry behind quark-gluons. By looking at the canonical representations of the Lorentz generators, we see that machine learning 'rediscovers' special relativity, purely from the data:

1. **Rotation Generators $L^{ij}$:**


$$
   (L^{12})^{\mu}_{\ \nu} = \begin{pmatrix}
   0 & 0 & 0 & 0 \\
   0 & 0 & 1 & 0 \\
   0 & -1 & 0 & 0 \\
   0 & 0 & 0 & 0
   \end{pmatrix},\;\;
    (L^{13})^{\mu}_{\ \nu} = \begin{pmatrix}
   0 & 0 & 0 & 0 \\
   0 & 0 & 0 & 1 \\
   0 & 0 & 0 & 0 \\
   0 & -1 & 0 & 0
   \end{pmatrix},\;\;
      (L^{23})^{\mu}_{\ \nu} = \begin{pmatrix}
   0 & 0 & 0 & 0 \\
   0 & 0 & 0 & 0 \\
   0 & 0 & 0 & 1 \\
   0 & 0 & -1 & 0
   \end{pmatrix}$$


2. **Boost Generators $L^{0i}$:**


$$\begin{align}
   (L^{01})^{\mu}_{\ \nu} = \begin{pmatrix}
   0 & 1 & 0 & 0 \\
   1 & 0 & 0 & 0 \\
   0 & 0 & 0 & 0 \\
   0 & 0 & 0 & 0
   \end{pmatrix},\;\;
   (L^{02})^{\mu}_{\ \nu} = \begin{pmatrix}
   0 & 0 & 1 & 0 \\
   0 & 0 & 0 & 0 \\
   1 & 0 & 0 & 0 \\
   0 & 0 & 0 & 0
   \end{pmatrix},\;\;
   (L^{03})^{\mu}_{\ \nu} = \begin{pmatrix}
   0 & 0 & 0 & 1 \\
   0 & 0 & 0 & 0 \\
   0 & 0 & 0 & 0 \\
   1 & 0 & 0 & 0
   \end{pmatrix}
\end{align}$$

Once we have the generators $L$ of the underlying group, we can extract the invariant metric tensor $J$ to further build invariant features, effectively baking invariance-equivariance into our Quantum Graph Neural Network (QGNN). To do so, we can solve for:

$$\begin{equation}
    L_{i}\cdot J + J^{T}\cdot L_{i} = 0
\end{equation}$$

Numerically, a small push from zero is enough to avoid getting the trivial solution $J = 0$:

$$\begin{equation}
\underset{J}{\operatorname{argmin}} \sum_{i=1}^{c} ||L_{i}\cdot J + J^{T}\cdot L_{i}||^{2} -a||J||^{2}
\end{equation}$$

The proof for this equation can be found in [1]. The code below implements this procedure and was kindly given by the authors:

Jet data
======

Classical Graph Neural Networks
======

Quantum Graph Neural Networks
======

Invariance/equivariance
======
