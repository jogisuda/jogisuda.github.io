---
title: 'Blog Post number 1'
date: 2012-08-14
permalink: /posts/2012/08/blog-post-1/
tags:
  - Geometric Quantum Machine Learning
---

Discovering new phenomena in particle physics, like the Higgs boson, involves the identification of rare signals that can shed light into unanswered questions about our universe. At the Large Hadron Collider (LHC), particle collisions produce vast amounts of data. In this blog post, I explain a project I'm developing for the Google Summer of Code 2024, under the Machine Learning for Science (ML4SCI) organization. I am developing a Lie-Equivariant Quantum Graph Neural Network (Lie-EQGNN), a quantum model that not only is data efficient, but also has symmetry-preserving properties from any arbitrary Lie algebra that can be learned directly from the data. Given the limitations by noise of current quantum hardware, efficiently designing new parameterized circuit architectures is crucial. This blog post highlights an overview of the project. Aiming for the best intuitive explanation, the sections are split into three parts:

* Introduction
* Group theory review
* Jet data
* A primer on quantum/classical graph neural networks.
* Ways to achieve equivariance/invariance.

Introduction
======

\par The story of symmetries and machine learning goes a long way, back to Minsky and Papert. They showed that if a neural network is invariant to some group, then its output could be expressed as functions of the orbits of the group. In recent years, the field of \textit{Geometric Deep Learning} was born, and it posits that in spite of the curse of dimensionality, the huge success of deep learning can be explained by two main inductive biases: symmetry and scale separation. By leveraging inherent symmetries in the data, the hypothesis space is effectively reduced, models can become more compact, and requiring less data for training - attributes that are particularly critical for quantum computing, given the current hardware limitations. 

\par In particle physics, symmetries underpin the fundamental laws governing the interactions and behaviors of subatomic particles: the Standard Model (SM), for example, is built upon symmetry principles such as gauge invariance, which dictate the interactions between particles and fields.

\par Recognizing the intertwined nature of symmetries and deep learning, a plethora of models have been developed that are invariant to different groups. Convolutional neural networks (CNNs), for example, which have revolutionized computer vision, are naturally invariant-equivariant to image translations. Transformer-based language models, on the other hand, exhibit permutation invariance. These architectures, when provided with appropriate data, are capable of learning stable latent representations under the action of their respective groups. An interesting question to be explored in quantum machine learning is the use of symmetries, especially in particle physics, which is replete with exotic symmetries. Quantum computers may have a unique advantage in this regard, as they can potentially capture quantum correlations more naturally and efficiently than classical computers. This can open the door to more accurate and insightful models that can better handle the complexities of particle interactions in huge datasets.

Group theory review
======
Symmetries are mathematically represented by algebraic structures called groups. Formally, a group $\mathcal{G}$ is a set endowed with a binary operation $\cdot$ $ : \mathcal{G} \times \mathcal{G} \rightarrow{\mathcal{G}}$, such that:

* $a\cdot (b \cdot c) = (a\cdot b) \cdot c, \forall a, b, c \in \mathcal{G}$ (associativity)
* $\forall g \in \mathcal{G}, \exists$ $I$ such that $g\cdot I = I\cdot g$ (existence of the neutral element)
* $\forall g \in \mathcal{G}, \exists$ $g^{-1}$ such that $g\cdot g^{-1} = g^{-1}\cdot g = I$ (the inverse element always exists)

Jet data
======

Classical Graph Neural Networks
======

Quantum Graph Neural Networks
======

Invariance/equivariance
======
